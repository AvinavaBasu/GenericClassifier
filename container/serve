#!/usr/bin/python3.6

"""
Copyright 2019 Parity Computing, Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"""

# NB: minor modifications to this file have been made for style and python3
# compatibility

# This file implements the scoring service shell.
# You don't necessarily need to modify it for various
# algorithms. It starts nginx and gunicorn with the
# correct configurations and then simply waits until
# gunicorn exits.
#
# The flask server is specified to be the app object in wsgi.py


#
# We set the following parameters:
#
# Parameter                Environment Variable              Default Value
# ---------                --------------------              -------------
# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores
# timeout                  MODEL_SERVER_TIMEOUT              60 seconds

import os
import signal
import subprocess
import sys

model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)
# notice that it might not be a good idea to run many services on the same node
# generic classifier is heavily parallelized so each service will attempt to use all
# available CPUs, running too many generic classifiers in parallel might not be optimal
model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', 1))


def sigterm_handler(nginx_pid, gunicorn_pid):
    try:
        os.kill(nginx_pid, signal.SIGQUIT)
    except OSError:
        pass
    try:
        os.kill(gunicorn_pid, signal.SIGTERM)
    except OSError:
        pass

    sys.exit(0)


def start_server():
    print(f'Starting the inference server with '
          f'{model_server_workers} workers and a timeout of {model_server_timeout}s.')

    # link the log streams to stdout/err so they will be logged to
    # the container logs
    subprocess.check_call(
        ['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])
    subprocess.check_call(
        ['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])

    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])
    gunicorn = subprocess.Popen(['gunicorn',
                                 '--timeout', str(model_server_timeout),
                                 '-k', 'sync',
                                 '-b', 'unix:/tmp/gunicorn.sock',
                                 '-w', str(model_server_workers),
                                 '--log-level', 'debug',
                                 'wsgi:app'])

    signal.signal(signal.SIGTERM,
                  lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))

    # If either subprocess exits, so do we.
    pids = set([nginx.pid, gunicorn.pid])
    while True:
        pid, _ = os.wait()
        if pid in pids:
            break

    sigterm_handler(nginx.pid, gunicorn.pid)
    print('Inference server exiting')


if __name__ == '__main__':
    # The main routine just invokes the start function.
    start_server()
